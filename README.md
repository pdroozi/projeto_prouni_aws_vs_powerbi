
# üöÄ Arquitetura 1: An√°lise Desktop (Power BI)

Esta abordagem foca na velocidade de prototipa√ß√£o e na an√°lise local, sendo a mais acess√≠vel para a maioria dos usu√°rios.

## 1. Processo de ETL (Extra√ß√£o, Transforma√ß√£o e Carga)

Os dados n√£o foram simplesmente "jogados" no Power BI. Foi necess√°rio um pr√©-processamento para unificar e tratar os dados:

* **Extra√ß√£o:** Os 3 *datasets* anuais (2017-2019) foram baixados do Portal de Dados Abertos do Governo.
* **Transforma√ß√£o:** Foi utilizado um script **Python** (`scripts/unificar_dados.py`) com a biblioteca **Pandas** para:
    1. Ler os 3 arquivos CSV separados (que estavam na pasta `/dados`).
    2. Concaten√°-los (empilhar) em um √∫nico DataFrame.
    3. Salvar o resultado como um √∫nico arquivo (`dados/ProUniTrienio.csv`) para ser usado como fonte de dados.
* **Carga:** O arquivo `ProUniTrienio.csv` foi carregado no Power BI.

## 2. Modelagem e An√°lise no Power BI

Dentro do Power BI, foi realizada a modelagem para criar os *insights* do dashboard:

* **Cria√ß√£o de Features (DAX):** A coluna `Idade Na Concessao` foi criada do zero usando **DAX**, subtraindo o ano de concess√£o do ano de nascimento (`ProUniTrienio[ANO_CONCESSAO_BOLSA] - YEAR(ProUniTrienio[DT_NASCIMENTO_BENEFICIARIO])`). Isso permitiu a an√°lise de "M√©dia de Idade" (KPI).
* **Limpeza (DAX):** A coluna `LocalizacaoMapa` foi criada com DAX (`'ProUniTrienio'[SIGLA_UF_BENEFICIARIO_BOLSA] & ", Brasil"`) para corrigir ambiguidades do motor de mapas (ex: "MT" = Mato Grosso, n√£o Montana/EUA).

## 3. Montagem e Justificativa dos Gr√°ficos (O Tutorial)

Cada visual foi escolhido para responder uma pergunta espec√≠fica. Abaixo est√° o "porqu√™" e o "como fazer" de cada um.

### KPI 1 e 2: Cart√µes (Resumo R√°pido)

* **Pergunta:** Qual o n√∫mero total de bolsas e a m√©dia de idade dos benefici√°rios?
* **Por qu√™:** Cart√µes s√£o usados para destacar os n√∫meros mais importantes (KPIs) do painel.
* **Como Fazer:**
    1. Adicione dois visuais de **"Cart√£o"**.
    2. **Cart√£o 1:** Arraste `CPF_BENEFICIARIO_BOLSA` para o campo "Valores" e mude a agrega√ß√£o para **"Contagem (Distinta)"**.
    3. **Cart√£o 2:** Arraste a coluna `Idade Na Concessao` (criada com DAX) para o campo "Valores" e mude a agrega√ß√£o para **"M√©dia"**.

### Filtro 1: Segmenta√ß√£o de Dados (O Filtro de Ano)

* **Pergunta:** Como posso ver os dados de um ano espec√≠fico?
* **Por qu√™:** A segmenta√ß√£o permite ao usu√°rio filtrar todo o painel, tornando o dashboard interativo.
* **Como Fazer:**
    1. Adicione um visual de **"Segmenta√ß√£o de Dados"**.
    2. Arraste `ANO_CONCESSAO_BOLSA` para o campo "Campo".
    3. Na formata√ß√£o do visual, mude o estilo para **"Bloco"** para criar os bot√µes.

### Gr√°fico 1: Mapa (Distribui√ß√£o Geogr√°fica)

* **Pergunta:** Onde as bolsas est√£o concentradas no Brasil?
* **Por qu√™:** Um mapa √© a forma mais intuitiva de mostrar dados geogr√°ficos.
* **Como Fazer:**
    1. Adicione um visual de **"Mapa"**.
    2. Arraste a coluna `LocalizacaoMapa` (criada com DAX) para o campo **"Localiza√ß√£o"**.
    3. Arraste `CPF_BENEFICIARIO_BOLSA` para o campo **"Tamanho da Bolha"**.
    4. Mude a agrega√ß√£o do `CPF` para **"Contagem (Distinta)"**.

### Gr√°fico 2 e 3: Gr√°fico de Barras (Top 10 Cursos e Universidades)

* **Pergunta:** Quais os 10 principais cursos e universidades?
* **Por qu√™:** Gr√°ficos de barras horizontais s√£o os melhores para "rankings" (Top 10), pois os nomes longos (cursos, IES) ficam f√°ceis de ler.
* **Como Fazer (Repita para `NOME_CURSO_BOLSA` e `NOME_IES_BOLSA`):**
    1. Adicione um visual de **"Gr√°fico de barras empilhadas"**.
    2. Arraste `NOME_CURSO_BOLSA` para o **"Eixo Y"**.
    3. Arraste `CPF_BENEFICIARIO_BOLSA` para o **"Eixo X"** (e mude para **"Contagem (Distinta)"**).
    4. No painel **"Filtros"**, expanda o filtro `NOME_CURSO_BOLSA`, mude o "Tipo de Filtro" para **"N superior"**, digite **10** em "Itens", e arraste `CPF_BENEFICIARIO_BOLSA` (Contagem Distinta) para o campo **"Por valor"**.

### Gr√°fico 4, 5 e 6: Gr√°fico de Rosca (Propor√ß√µes)

* **Pergunta:** Qual a propor√ß√£o de bolsas por Ra√ßa, Tipo e Modelo de Ensino?
* **Por qu√™:** Gr√°ficos de rosca (ou pizza) s√£o perfeitos para mostrar a composi√ß√£o percentual (partes de um todo) de forma simples.
* **Como Fazer (Repita para `RACA_BENEFICIARIO_BOLSA`, `TIPO_BOLSA`, `MODALIDADE_ENSINO_BOLSA`):**
    1. Adicione um visual de **"Gr√°fico de Rosca"**.
    2. Arraste `RACA_BENEFICIARIO_BOLSA` para a **"Legenda"**.
    3. Arraste `CPF_BENEFICIARIO_BOLSA` para os **"Valores"** (e mude para **"Contagem (Distinta)"**).

## 4. Resultado (Dashboard)

![Dashboard Power BI](power_bi_local/Dashboard_ProUni.jpg)

(O arquivo .pbix interativo est√° na pasta /power_bi_local)

# ‚òÅÔ∏è Arquitetura 2: Pipeline de Nuvem (AWS)

Esta abordagem foca em uma solu√ß√£o escal√°vel, automatiz√°vel e padr√£o de mercado, capaz de lidar com volumes de dados massivos ‚Äî exatamente como o nosso arquivo de 108 MB.

O tutorial a seguir demonstra como criar um pipeline de dados "Serverless" (sem servidor) usando os servi√ßos da AWS.

## 1. Ingest√£o de Dados (AWS S3)

O primeiro passo √© mover nossos dados locais para a nuvem. Usamos o **AWS S3 (Simple Storage Service)**, um servi√ßo de armazenamento de objetos.

* **A√ß√£o:** Um script Python (`scripts/upload_para_s3.py`) usando a biblioteca `boto3` foi criado para automatizar o processo.
* **O que ele faz:**
    1.  Verifica se o bucket (ex: `pdroozi-prouni-projeto-extensao`) existe.
    2.  Se n√£o existir, ele o **cria automaticamente** na regi√£o especificada (ex: `us-east-1`).
    3.  Faz o upload do arquivo `dados/ProUniTrienio.csv` para dentro do bucket.

### 2. O Processo (A Prova)

Abaixo est√° o passo a passo documentado do processo de ingest√£o:

**Passo 1: O Console da AWS**
O primeiro contato com o console da AWS, onde os servi√ßos s√£o gerenciados.

![Painel AWS Console](aws-pipeline/img/painel_aws_console.png)

**Passo 2: O S3 Vazio**
O servi√ßo S3 antes da execu√ß√£o do nosso script, ainda sem "buckets" (locais de armazenamento).

![S3 Vazio](aws-pipeline/img/s3_vazio_ainda_sem_bucket.png)

**Passo 3: A Execu√ß√£o do Script**
O log do terminal do VS Code, mostrando o script `upload_para_s3.py` sendo executado e confirmando a cria√ß√£o do bucket e o in√≠cio do upload.

![Log de Upload S3](aws-pipeline/img/upload_para_s3.png)

**Passo 4: O Bucket Criado**
Ap√≥s o script, o bucket `bucket_pdroozi_projeto_prouni` agora existe no S3.

![Bucket Criado](aws-pipeline/img/bucket_criado.png)

**Passo 5: O Arquivo CSV Uploadado**
A prova final: nosso arquivo `ProUniTrienio.csv` (108.7 MB) est√° agora armazenado de forma segura na nuvem, dentro do bucket.

![Arquivo CSV no S3](aws-pipeline/img/arquivo_csv_ProUniTrienio_uploadado.png)

---

## 2. Cat√°logo de Dados (AWS Glue)

Com o arquivo no S3, precisamos de um m√©todo *Serverless* para ler e entender a estrutura do CSV de 108 MB. Usamos o **AWS Glue** para criar um Cat√°logo de Dados (metadados) sobre o arquivo, sem precisar carregar ele em um banco de dados tradicional.

* **A√ß√£o:** Criamos um **Glue Crawler** (Rastreador), que √© uma ferramenta automatizada de descoberta de esquema.
* **O que ele faz:**
    1.  O Crawler "l√™" o arquivo `ProUniTrienio.csv` diretamente do S3.
    2.  Ele analisa os dados para **detectar automaticamente o esquema** (nomes das colunas e tipos de dados).
    3.  Ele salva esse esquema como uma nova "tabela" em um "Banco de dados Glue".

### O Processo (Tutorial Passo a Passo)

| A√ß√£o | Descri√ß√£o | Imagem |
| :--- | :--- | :--- |
| **Console Glue** | Primeiro acesso ao servi√ßo AWS Glue. | ![Painel AWS Glue](aws-pipeline/img/painel_aws_glue.png) |
| **Criar Crawler** | In√≠cio da configura√ß√£o do novo Crawler. | ![Criar Crawler](aws-pipeline/img/create_crawler.png) |
| **Propriedades** | Defini√ß√£o do nome do Crawler (ex: `prouni-csv-crawler`). | ![Propriedades do Crawler](aws-pipeline/img/crawler_properties.png) |
| **Fonte de Dados** | Apontando o Crawler para o caminho exato do bucket S3 (`s3://.../dados/`). | ![Sele√ß√£o da Fonte de Dados](aws-pipeline/img/select_data_source.png) |
| **IAM Role** | Cria√ß√£o do IAM Role com permiss√µes de leitura no S3, essencial para o sucesso da execu√ß√£o. | ![Sele√ß√£o do Role IAM](aws-pipeline/img/select_role.png) |
| **Database/DB** | Defini√ß√£o de onde a tabela de esquema ser√° salva (ex: no `prouni_db`). | ![Sele√ß√£o do Banco de Dados](aws-pipeline/img/select_db.png) |
| **Crawler Criado** | O Crawler pronto para a execu√ß√£o. | ![Crawler Criado](aws-pipeline/img/crawler_created.png) |
| **Execu√ß√£o** | In√≠cio manual da execu√ß√£o do Crawler. | ![Execu√ß√£o do Crawler](aws-pipeline/img/crawler_run.png) |
| **Conclus√£o** | O Crawler finaliza a varredura do arquivo e gera o esquema. | ![Crawler Conclu√≠do](aws-pipeline/img/crawler_sucess.png) |
| **Tabela Final** | **Prova:** A tabela `prouni_dados` (ou similar) √© criada no Cat√°logo de Dados, contendo o esquema de colunas do nosso CSV. | ![Tabela Criada](aws-pipeline/img/crawler_table_created.png) |

---

## 3. Transforma√ß√£o e Consulta (AWS Athena)

Com a tabela catalogada, usamos o **AWS Athena** para rodar consultas SQL diretamente sobre os arquivos no S3. Este servi√ßo atua como nossa camada de **Transforma√ß√£o (T)** no pipeline de nuvem, replicando as colunas calculadas do Power BI.

* **A√ß√£o:** Criamos consultas SQL para gerar novas "features" (colunas calculadas) em tempo real, sem alterar o arquivo original no S3.
* **Ferramenta:** Usamos o Editor de Consultas do Athena (interface web).

### O Processo (A Prova)

| A√ß√£o | Descri√ß√£o | Imagem |
| :--- | :--- | :--- |
| **Painel Athena** | Primeiro contato com a interface do Athena, com a tabela do Glue j√° dispon√≠vel. | ![Painel AWS Athena](aws-pipeline/img/painel_aws_athena.png) |
| **Transforma√ß√£o 1: Idade** | Cria√ß√£o da coluna `idade_na_concessao` usando as fun√ß√µes `CAST` e `SUBSTR` do SQL para replicar a l√≥gica do DAX. | ![Cria√ß√£o da Coluna Idade](aws-pipeline/img/create_Idade.png) |
| **Transforma√ß√£o 2: Localiza√ß√£o** | Cria√ß√£o da coluna `localizacao_mapa` usando a fun√ß√£o SQL `CONCAT` para resolver o problema de geocodifica√ß√£o do mapa. | ![Cria√ß√£o da Coluna Localiza√ß√£o](aws-pipeline/img/create_LocalizacaoMapa.png) |
| **Consulta Final** | A consulta SQL que une as duas transforma√ß√µes e seleciona os dados prontos para o QuickSight. Esta √© a nossa *fonte de dados final* na arquitetura AWS. | ![Consulta Final para BI](aws-pipeline/img/create_final_query.png) |

---
## 4. Visualiza√ß√£o Final (AWS QuickSight)

O √∫ltimo passo do pipeline de nuvem √© a visualiza√ß√£o dos dados transformados.

* **A√ß√£o:** O **AWS QuickSight** foi utilizado para conectar-se √† consulta SQL final no Athena.
* **Conex√£o:** O QuickSight usa a camada de Athena para ler os dados, garantindo que o dashboard seja sempre atualizado e escal√°vel.

### O Processo de Conex√£o e An√°lise (Tutorial Passo a Passo)

| A√ß√£o | Descri√ß√£o | Imagem |
| :--- | :--- | :--- |
| **Painel QuickSight** | Acessando o console para iniciar a an√°lise. | ![Painel AWS QuickSight](aws-pipeline/img/painel_aws_quicksight.png) |
| **P√°gina Inicial** | A p√°gina inicial do QuickSight, onde as an√°lises s√£o criadas. | ![P√°gina Principal QuickSuite](aws-pipeline/img/mainpage_quicksuite.png) |
| **Criar An√°lise** | In√≠cio do processo para criar uma nova visualiza√ß√£o. | ![Criar An√°lise](aws-pipeline/img/suite_create_analysis.png) |
| **Fonte de Dados** | Escolha da fonte de dados **Athena**, que consulta nosso Cat√°logo Glue. | ![Escolha da Fonte de Dados](aws-pipeline/img/suite_data_source.png) |
| **Cat√°logo/Tabela** | Sele√ß√£o do `prouni_db` (nosso Cat√°logo Glue). | ![Sele√ß√£o de Tabelas](aws-pipeline/img/suite_choose_tables.png) |
| **SQL Personalizado** | Colagem da consulta SQL final do Athena, criando as colunas calculadas (Idade e Localiza√ß√£o L√≥gica). | ![Colar Consulta Customizada](aws-pipeline/img/suite_custom_query.png) |
| **Confirma√ß√£o** | Confirma√ß√£o da conex√£o e entrada na tela de Prepara√ß√£o de Dados (Data Prep). | ![Confirma√ß√£o Data Prep](aws-pipeline/img/suite_confirm_data_prep.png) |
| **P√°gina de An√°lise** | A interface de Business Intelligence (BI) para montagem do dashboard. | ![P√°gina de BI](aws-pipeline/img/suite_business_inteligence_page.png) |
| **Resultado Final** | O dashboard QuickSight com os visuais de KPI, Propor√ß√£o e Ranking replicados. | ![Dashboard QuickSight Final](aws-pipeline/img/suite_dashboard.png) |

---
## Conclus√£o e Valor do Projeto

Este projeto demonstrou que, embora o **Power BI** seja superior em velocidade e facilidade de prototipa√ß√£o, a arquitetura **AWS** (S3 > Glue > Athena > QuickSight) √© a solu√ß√£o ideal para grandes volumes de dados e para a cria√ß√£o de um pipeline de dados *serverless* e profissional, alinhando o aprendizado t√©cnico com o requisito de extens√£o da UNIP atrav√©s deste tutorial comparativo.
